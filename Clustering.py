# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m-XAFap8TNPyFpPTIz6esBnZRSPTPtgY
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, MeanShift
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.datasets import load_iris

# Load Dataset
data = load_iris()
df = pd.DataFrame(data.data, columns=data.feature_names)

# Preprocessing Methods
def preprocess_data(df, method):
    if method == 'Standardization':
        return StandardScaler().fit_transform(df)
    elif method == 'Normalization':
        return MinMaxScaler().fit_transform(df)
    elif method == 'PCA':
        return PCA(n_components=2).fit_transform(StandardScaler().fit_transform(df))
    return df

preprocessing_methods = ['No Processing', 'Standardization', 'Normalization', 'PCA', 'Transform', 'T+N', 'T+N+PCA']

# Clustering Algorithms
clustering_algorithms = {
    'K-Means': KMeans,
    'Hierarchical': AgglomerativeClustering,
    'Mean-Shift': MeanShift
}

# Results Storage
results = []

for method in preprocessing_methods:
    processed_data = preprocess_data(df, method) if method in ['Standardization', 'Normalization', 'PCA'] else df

    for algo_name, algo in clustering_algorithms.items():
        for clusters in [3, 4, 5]:
            if algo_name == 'Mean-Shift':
                model = algo()
            else:
                model = algo(n_clusters=clusters)
            labels = model.fit_predict(processed_data)

            # Compute Evaluation Metrics
            silhouette = silhouette_score(processed_data, labels) if len(set(labels)) > 1 else 'N/A'
            calinski = calinski_harabasz_score(processed_data, labels) if len(set(labels)) > 1 else 'N/A'
            davies = davies_bouldin_score(processed_data, labels) if len(set(labels)) > 1 else 'N/A'

            results.append([algo_name, method, clusters, silhouette, calinski, davies])

# Convert results to DataFrame
results_df = pd.DataFrame(results, columns=['Algorithm', 'Preprocessing', 'Clusters', 'Silhouette Score', 'Calinski-Harabasz', 'Davies-Bouldin'])
print(results_df)

# Save results to CSV
results_df.to_csv("clustering_results.csv", index=False)

# Format as LaTeX Table
latex_table = results_df.to_latex(index=False, float_format="%.2f")
with open("clustering_results.tex", "w") as f:
    f.write(latex_table)

